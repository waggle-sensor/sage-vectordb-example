# Blades
# TODO: seperate the requirements for different GPUs
#----------------------------- 
# transformers==4.44.* # <--- for florence2base
# huggingface_hub[cli]==0.24.* # <--- for florence2base
# transformers==4.52.1 # <--- qwen2.5 needs this version
# autoawq>=0.1.8 # <--- for awq quantization used on qwen2.5
# bitsandbytes==0.43.2 # <--- for qwen2.5 bnb quantization
# accelerate>=0.26.0 # <--- for loading weights on gpu for qwen2.5
# numpy<2 # <--- numpy 2.0 is not compatible with transformers
# huggingface_hub[cli]==0.30.* # <--- upgraded for transformers==4.51.3
# hf_xet # better performance for large files
# Pillow==10.4.*
# timm==1.0.*
# einops==0.8.*
# packaging==24.2.* #enable for flash attention, flash attention must have have CUDA 11.7 and above 

#Gemma3 with H100
#-----------------------------
#transformers==4.52.1 # <--- qwen2.5 and gemma 3 needs this version
#numpy<2 # <--- numpy 2.0 is not compatible with transformers
#huggingface_hub[cli]==0.30.* # <--- upgraded for transformers==4.51.3
#hf_xet # better performance for large files
#Pillow==10.4.*
#timm==1.0.*
#einops==0.8.*
# packaging==24.2.* #enable for flash attention, flash attention must have have CUDA 11.7 and above

# Gemma3 with amd64
#-----------------------------
transformers==4.52.1 # <--- qwen2.5 and gemma 3 needs this version
numpy<2 # <--- numpy 2.0 is not compatible with transformers
huggingface_hub[cli]==0.30.* # <--- upgraded for transformers==4.51.3
hf_xet # better performance for large files
Pillow==10.4.*
timm==1.0.*
einops==0.8.*
triton==3.0.0
# packaging==24.2.* #enable for flash attention, flash attention must have have CUDA 11.7 and above
