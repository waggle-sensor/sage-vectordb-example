# Blades
# TODO: seperate the requirements for different GPUs
#----------------------------- 
# transformers==4.44.* # <--- for florence2base
# huggingface_hub[cli]==0.24.* # <--- for florence2base
# transformers==4.52.1 # <--- qwen2.5 needs this version
# autoawq>=0.1.8 # <--- for awq quantization used on qwen2.5
# bitsandbytes==0.43.2 # <--- for qwen2.5 bnb quantization
# accelerate>=0.26.0 # <--- for loading weights on gpu for qwen2.5
# numpy<2 # <--- numpy 2.0 is not compatible with transformers
# huggingface_hub[cli]==0.30.* # <--- upgraded for transformers==4.51.3
# hf_xet # better performance for large files
# Pillow==10.4.*
# timm==1.0.*
# einops==0.8.*
# packaging==24.2.* #enable for flash attention, flash attention must have have CUDA 11.7 and above 

#H100
#-----------------------------
# transformers==4.44.* # <--- for florence2base
# huggingface_hub[cli]==0.24.* # <--- for florence2base
transformers==4.52.1 # <--- qwen2.5 needs this version
accelerate>=0.26.0 # <--- for loading weights on gpu for qwen2.5
numpy<2 # <--- numpy 2.0 is not compatible with transformers
huggingface_hub[cli]==0.30.* # <--- upgraded for transformers==4.51.3
hf_xet # better performance for large files
Pillow==10.4.*
timm==1.0.*
einops==0.8.*
# packaging==24.2.* #enable for flash attention, flash attention must have have CUDA 11.7 and above 